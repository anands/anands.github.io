<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python | Anand Sudhanaboina]]></title>
  <link href="https://anands.github.io/blog/categories/python/atom.xml" rel="self"/>
  <link href="https://anands.github.io/"/>
  <updated>2020-07-01T14:43:52+05:30</updated>
  <id>https://anands.github.io/</id>
  <author>
    <name><![CDATA[Anand Sudhanaboina]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Outlier Detection Using Python]]></title>
    <link href="https://anands.github.io/blog/2015/11/26/outlier-detection-using-python/"/>
    <updated>2015-11-26T11:58:43+05:30</updated>
    <id>https://anands.github.io/blog/2015/11/26/outlier-detection-using-python</id>
    <content type="html"><![CDATA[<p>Before writing code I would like to emphasize the difference between anomaly and a outlier:</p>

<ul>
<li><strong>Outlier:</strong> Legitimate data point that’s far away from the mean or median in a distribution.</li>
<li><strong>Anomaly:</strong> Illegitimate data point that’s generated by a different process than whatever generated the rest of the data.</li>
</ul>


<p>Outlier detection varies between single dataset and multiple datasets. In <strong>single dataset outlier detection</strong> we figure out the outliers within the dataset. We can do this by using two methods, <a href="https://en.wikipedia.org/wiki/Median_absolute_deviation">Median Absolute Deviation (MAD)</a> and <a href="https://en.wikipedia.org/wiki/Standard_deviation%20https://en.wikipedia.org/wiki/Cosine_similarity">Standard deviation (SD)</a>. Though MAD and SD give different results they are intended to do the same work. I’m not explaining the mathematical expressions as you can find them from wikipedia.</p>

<p>Let’s consider a sample dataset:</p>

<p><img src="/images/OjYLGUW.png" alt="dataset" /></p>

<p>I’ve written a Python script using numpy library, this script calculates both MAD and SD:</p>

<pre><code class="python">from __future__ import division
import numpy

# Sample Dataset
x = [10, 9, 13, 14, 15,8, 9, 10, 11, 12, 9, 0, 8, 8, 25,9,11,10]

# Median absolute deviation
def mad(data, axis=None):
    return numpy.mean(numpy.abs(data - numpy.mean(data, axis)), axis)
_mad = numpy.abs(x - numpy.median(x)) / mad(x)

# Standard deviation
_sd = numpy.abs(x - numpy.mean(x)) / numpy.std(x)

print _mad
print _sd
</code></pre>

<p>Let’s visualize the output:</p>

<p><img src="/images/0eylz1t.png" alt="visualize" /></p>

<p>It’s clear that we’ve detected a spike if there is a change in the dataset. After comparing results of several datasets I would like to mention MAD is more sensitive when compared to SD, but more computing intensive. I’ve experimented the same code with 1 M data points, SD performed near to 2x when compared with MAD.</p>

<p><img src="/images/zkB3Hyu.png" alt="mad-sd" /></p>

<p><strong>Multiple dataset outlier detection:</strong> In this we figure out anomaly in different datasets when compared with target dataset. For example, say you have data of your web site traffic on hourly basis for 10 days including today, and you would like to figure out if there is an outlier in today’s data when compared with other 9 days data. I’ve done this using <a href="https://en.wikipedia.org/wiki/Mahalanobis_distance">Mahalanobis distance algorithm</a> and implemented using Python with numpy.</p>

<p>Let’s consider sample dataset:</p>

<p><img src="/images/32tuL2E.png" alt="multiple-dataset" /></p>

<p>The highlighted path is the target dataset.Let’s feed this to the algorithm:</p>

<pre><code class="python">from __future__ import division
import numpy as np

# Base dataset
dataset = np.array(
        [
            [9,9,10,11,12,13,14,15,16,17,18,19,18,17,11,10,8,7,8],
            [8,6,10,13,12,11,12,12,13,14,1,16,20,21,19,18,11,5,5],
        ])

# target: dataset to be compared
target = [0,0,0,0,10,9,15,11,15,17,13,14,18,17,14,22,11,5,5]

# Avg of SD of each dataset
dataset_std = dataset.std()

# Avg of arrays in dataset
dataset_sum_avg = np.array([0] * len(dataset[0])) # Create a empty dataset
for data in dataset:
    dataset_sum_avg = dataset_sum_avg + ( data / len(dataset)) # Add up all datapoints of dataset

# Substract the target dataset with avg of datapoints sum and divide by SD
data = np.abs(target - dataset_sum_avg) / dataset_std

print data
</code></pre>

<p>This gives us the outliers, on visualizing the result we get:</p>

<p><img src="/images/18reLxS.png" alt="image" /></p>

<p>If you have a look on the data we fed into the algorithm it’s clear that we are able to detect the outliers for today’s input when compared to other x days.</p>

<p>Feel free to explore are a few other algorithms <a href="https://en.wikipedia.org/wiki/Cosine_similarity">Cosine similarity</a>, <a href="https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient">Sørensen–Dice coefficient</a>, <a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard index</a>, <a href="https://en.wikipedia.org/wiki/SimRank">SimRank</a> and others.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Remote Logging With Python]]></title>
    <link href="https://anands.github.io/blog/2015/11/03/remote-logging-with-python/"/>
    <updated>2015-11-03T11:50:48+05:30</updated>
    <id>https://anands.github.io/blog/2015/11/03/remote-logging-with-python</id>
    <content type="html"><![CDATA[<p>Debugging logs can be formidable task if you run same service on multiple production loads with local logging behind a load balancer, you are only left one option, ssh into different servers and then debug the logs.</p>

<p>Logging to a single server from multiple servers can simply debugging, Python provides a in built functionality for logging, by just adding a few lines to the logging config you will be able to send the log to a remote server and then your remote server need to handle this request. In remote server you can store this logs into flat files or NoSQL</p>

<p>A rudimentary architecture would be:</p>

<p><img src="/images/NQOfEuX.png" alt="architecture" /></p>

<p>I’ve created a few code samples to get this done:</p>

<p>Configure a <em>HTTPHandler</em> to the logging handler to send logs to remote server instead of local tty:</p>

<pre><code class="python">import logging
import logging.handlers
logger = logging.getLogger('Synchronous Logging')
http_handler = logging.handlers.HTTPHandler(
    '127.0.0.1:3000',
    '/log',
    method='POST',
)
logger.addHandler(http_handler)

# Log messages:
logger.warn('Hey log a warning')
logger.error("Hey log a error")
</code></pre>

<p>On the logging server, I’ve created a simple flask application which can handle a post request:</p>

<pre><code class="python">from flask import Flask, request
import json

app = Flask(__name__)

@app.route('/log',methods=['POST'])
def index():
    print json.dumps(request.form)
    return ""

if __name__ == '__main__':
    app.run(host='0.0.0.0', port = 3000, debug=True)
</code></pre>

<p>Assuming the server is up and you send a log request, this is how the log structure looks:</p>

<pre><code class="json">{
    "relativeCreated": "52.1631240845",
    "process": "10204",
    "args": "()",
    "module": "km",
    "funcName": "&lt;module&gt;",
    "exc_text": "None",
    "name": "Synchronous Logging",
    "thread": "139819818469184",
    "created": "1446532937.04",
    "threadName": "MainThread",
    "msecs": "37.367105484",
    "filename": "km.py",
    "levelno": "40",
    "processName": "MainProcess",
    "pathname": "km.py",
    "lineno": "13",
    "msg": "Hey log a error",
    "exc_info": "None",
    "levelname": "ERROR"
}
</code></pre>

<p>Important properties of this structure would be msg, name &amp; level. Property name is what you pass to getLogger function, and level would the level of logging (error = 40, warning = 30, etc).</p>

<p>This approach is synchronized, if you want logging to be async use threads:</p>

<pre><code class="python">import logging, thread, time
import logging.handlers
logger = logging.getLogger('Asynchronous Logging') # Name
http_handler = logging.handlers.HTTPHandler(
    '127.0.0.1:3000',
    '/log',
    method='POST',
)
logger.addHandler(http_handler)
thread.start_new_thread( logger.error, ("Log error",))
time.sleep(1) # Just to keep main thread alive.
</code></pre>

<p>By this way we need not bother about storage of application server (If you are not storing any data to FS then logs would be the only thing) and debugging would be easy.</p>

<p>Save to mongo to perform analytics and / or to perform quick queries:</p>

<pre><code class="python">from flask import Flask, request
import json
from pymongo import MongoClient

app = Flask(__name__)

# Mongo setup:
client = MongoClient()
db = client['logs']
collection = db['testlog']

@app.route('/log',methods=['POST'])
def index():
    # Convert form POST object into a representation suitable for mongodb
    data = json.loads(json.dumps(request.form))
    response = collection.insert_one(data)
    print response.inserted_id
    return ""

if __name__ == '__main__':
    app.run(host='0.0.0.0', port = 3000, debug=True)
</code></pre>
]]></content>
  </entry>
  
</feed>
