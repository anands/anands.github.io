
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <title>Streamline Log Analysis &amp; Analytics - Anand Sudhanaboina</title>
  
  <meta name="author" content="Anand Sudhanaboina">

  
  <meta name="description" content="Unified logging is essential when you are scaling your application, this helps in grouping the logs on component (service) level and also providing &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="https://anands.github.io/blog/2017/01/07/streamline-log-analysis-and-analytics/">
  <link href="/avatar.jpg" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Anand Sudhanaboina" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<script>
(function(i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function() {
        (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date();
    a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
})(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

ga('create', 'UA-64094784-2', 'auto');
ga('send', 'pageview');
</script>

  

</head>

<body   >
  <header role="banner"><hgroup>
    <h1><a href="/">Anand Sudhanaboina</a></h1> 
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<ul class="main-navigation">
    <li><a href="/">Home</a></li>
    <li><a href="/blog">Blog</a></li>
    <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Streamline Log Analysis &amp; Analytics</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-01-07T11:03:13+05:30'><span class='date'><span class='date-month'>Jan</span> <span class='date-day'>7</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>11:03 am</span></time>
        
      </p>
    
  </header>


<div class="entry-content"><p>Unified logging is essential when you are scaling your application, this helps in grouping the logs on component (service) level and also providing search capability on multiple services, for example: assume that you have a subscription service which have two internal SOA services, payment service and web service, if the logs are scattered and also assuming that these services are horizontally scaled you will be having hard time to debug these logs, instead if you have unified logging in place, a search on unique indentifier will get the results from all the services, this helps us in quick resolution with less effort. This article will demonstrate a POC which is built using multiple FOSS and with zero custom code to streamline unified logging. Alternatively you might be interested in having a look at <a href="https://www.loggly.com/">Loggly</a>, <a href="https://www.sumologic.com/">Sumo Logic</a> and <a href="https://www.splunk.com/">Splunk</a></p>

<p>FOSS used:</p>

<ul>
<li><a href="https://github.com/influxdata/telegraf">Telegraf</a></li>
<li><a href="http://kafka.apache.org/">Kafka</a></li>
<li><a href="https://www.elastic.co/products/logstash">Logstash</a></li>
<li><a href="https://www.elastic.co/products/elasticsearch">ElasticSearch</a></li>
<li><a href="https://www.elastic.co/products/kibana">Kibana</a> / <a href="http://grafana.org/">Grafana</a> (Either or both)</li>
</ul>


<p>Architecture design:</p>

<p><img src="/images/log-analysis.svg" alt="" /></p>

<p>Working mechanism overview:</p>

<ol>
<li>Telegraf running on VM instances will <a href="https://github.com/influxdata/telegraf/tree/master/plugins/inputs/logparser">push logs</a> to <a href="https://github.com/influxdata/telegraf/tree/master/plugins/outputs/kafka">Kafka</a></li>
<li>Logstash will:

<ul>
<li>Read data from Kafka</li>
<li>Modify data if required</li>
<li>Presist data to ElasticSearch</li>
</ul>
</li>
<li>Grafana and / or Kibana will read fetch data from ES based on the queries</li>
</ol>


<p><em>In this example I&rsquo;m using Apache access logs as my source.</em></p>

<p><strong>Step 1: Setup Telegraf:</strong></p>

<p><a href="https://gist.github.com/anands/730207cc3543a477a14fb5b007442042#file-telegraf_installation-txt">Download and install Telegraf if you don&rsquo;t have one running</a>.</p>

<p>Below is the config which you need to add to telegraf.conf (/etc/telegraf/telegraf.conf):</p>

<p><a href="https://github.com/influxdata/telegraf/tree/master/plugins/inputs/logparser">Log parser input plugin</a> config:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>[[inputs.logparser]]
</span><span class='line'>  ## files to tail.
</span><span class='line'>  files = [&quot;/var/log/apache2/access.log&quot;]
</span><span class='line'>  ## Read file from beginning.
</span><span class='line'>  from_beginning = true
</span><span class='line'>  name_override = &quot;apache_access_log&quot;
</span><span class='line'>  ## For parsing logstash-style &quot;grok&quot; patterns:
</span><span class='line'>  [inputs.logparser.grok]
</span><span class='line'>    patterns = [&quot;%{COMMON_LOG_FORMAT}&quot;]
</span></code></pre></td></tr></table></div></figure>


<p><a href="https://github.com/influxdata/telegraf/tree/master/plugins/outputs/kafka">Kafka output plugin</a> config:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>[[outputs.kafka]]
</span><span class='line'>  brokers = [&quot;localhost:9092&quot;]
</span><span class='line'>  topic = &quot;logparse&quot;
</span><span class='line'>  compression_codec = 0
</span><span class='line'>  required_acks = -1
</span><span class='line'>  max_retry = 5
</span><span class='line'>  data_format = &quot;json&quot;
</span></code></pre></td></tr></table></div></figure>


<p>If you are not using Telegraf before and just want to test this out, use this (<a href="https://gist.github.com/anands/730207cc3543a477a14fb5b007442042#file-telegraf-conf">telegraf.conf</a>) config file.</p>

<p><strong>Step 3: Setup Kafka:</strong></p>

<p><a href="https://gist.github.com/anands/730207cc3543a477a14fb5b007442042#file-kafka_installation-txt">Download and start Kafka if you don&rsquo;t have one running</a></p>

<p>Create a Kafka topic using the command:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>bin/kafka-topics.sh --create --topic logparse --zookeeper localhost:2181 --partitions <span class="m">1</span> --replication-factor 1
</span></code></pre></td></tr></table></div></figure>


<p>Feel free to change the Kafka topic, partitions and replication according to your needs, for example: topics logs-web, logs-payments can be used with different partitions and avaliablity.</p>

<p><strong>Step 4: Setup ElasticSearch:</strong></p>

<p><a href="https://gist.github.com/anands/730207cc3543a477a14fb5b007442042#file-es_installation-txt">Download and start ElasticSearch</a></p>

<p><strong>Step 4: Setup Logstash</strong></p>

<p>For now, I want to analyse the HTTP response codes hence I changed the logstash config accordingly, below is the config:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>input {
</span><span class='line'>  kafka {
</span><span class='line'>      topics =&gt; [&#39;logparse&#39;]
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'>  elasticsearch {
</span><span class='line'>      codec =&gt; &#39;json&#39;
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>filter {
</span><span class='line'>  json {
</span><span class='line'>      source =&gt; &#39;message&#39;
</span><span class='line'>      remove_field =&gt; [&#39;message&#39;]
</span><span class='line'>  }
</span><span class='line'>  mutate {
</span><span class='line'>      add_field =&gt; {&quot;resp_code&quot; =&gt; &quot;%{[tags][1][1]}&quot;}
</span><span class='line'>      }
</span><span class='line'>      mutate {
</span><span class='line'>          convert =&gt; { &quot;resp_code&quot; =&gt; &quot;integer&quot; }
</span><span class='line'>      }
</span><span class='line'>}
</span></code></pre></td></tr></table></div></figure>


<p>Save this config to a file (logstash-test.yml) and start the logstash:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>bin/logstash -f config/logstash-test.yml
</span></code></pre></td></tr></table></div></figure>


<p>References:</p>

<ul>
<li><a href="https://www.elastic.co/guide/en/logstash/current/index.html">Logstash reference guide</a></li>
</ul>


<p><strong>Step 5: Test the flow</strong></p>

<p>Start the telegraf using the command <code>telegraf</code> and make random HTTP requests to the Apache server and see if the data is being presisted to ES.</p>

<p>Here are a few resources:</p>

<ul>
<li>Telegraf log location: /tmp/telegraf.log (If you used the config I used)</li>
<li>Get list of ES indices - <a href="http://localhost:9200/_cat/indices?v">http://localhost:9200/_cat/indices?v</a></li>
<li>Get data of a index - <a href="http://localhost:9200/logstash-2017.01.08/logs/_search?size=1000&amp;pretty=1">http://localhost:9200/logstash-2017.01.08/logs/_search?size=1000&amp;pretty=1</a></li>
</ul>


<p>If everything goes as expected these are a few thing which you should be seeing:</p>

<p>Telegraf writing to Kafka:</p>

<p><img src="/images/la-e1.png" alt="" /></p>

<p>Data in ES:</p>

<p><img src="/images/la-e2.png" alt="" /></p>

<p><strong>Step 6: Setup Grafana:</strong></p>

<ul>
<li><a href="https://gist.github.com/anands/730207cc3543a477a14fb5b007442042#file-grafana_installation-txt">Install and start Grafana</a></li>
<li>Add ES as data source in Grafana</li>
<li>Add charts and queries</li>
<li>Below is my Grafana board with monitoring of status codes 200 &amp; 400 (Looks good if you have more data):</li>
</ul>


<p><img src="/images/la-fullscreen-grafana.png" alt="" /></p>

<p>For Kibana, download and start Kibana, and then add ES as data source and execute queries.</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Anand Sudhanaboina</span></span>

      




<time class='entry-date' datetime='2017-01-07T11:03:13+05:30'><span class='date'><span class='date-month'>Jan</span> <span class='date-day'>7</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>11:03 am</span></time>
      


    </p>
    
      <div class="sharing">
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2016/11/20/continuous-backup-of-dot-files-using-python-and-dropbox/" title="Previous Post: Backup of Dot Files Using Python & Dropbox">&laquo; Backup of Dot Files Using Python & Dropbox</a>
      
      
        <a class="basic-alignment right" href="/blog/2017/04/10/launch-random-chrome-instances-faster/" title="Next Post: Launch Random Chrome Instances Faster">Launch Random Chrome Instances Faster &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2018/11/18/serve-static-pages-on-s3-without-html-extension/">Serve Static Pages on S3 Without .html Extension</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/09/16/s3-cname-ssl-with-cloudflare/">S3 CNAME SSL With CloudFlare</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/04/10/launch-random-chrome-instances-faster/">Launch Random Chrome Instances Faster</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/01/07/streamline-log-analysis-and-analytics/">Streamline Log Analysis &amp; Analytics</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/11/20/continuous-backup-of-dot-files-using-python-and-dropbox/">Backup of Dot Files Using Python & Dropbox</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/01/20/network-performance-analysis-using-curl/">Network Performance Analysis Using CURL</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/12/01/distributed-log-search-using-gnu-parallel/">Distributed Log Search Using GNU Parallel</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/11/30/reddits-ranking-algorithm/">Reddit's Ranking Algorithm</a>
      </li>
    
  </ul>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2011 - 2020 - Anand Sudhanaboina.  
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'codehatecom';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'https://anands.github.io/blog/2017/01/07/streamline-log-analysis-and-analytics/';
        var disqus_url = 'https://anands.github.io/blog/2017/01/07/streamline-log-analysis-and-analytics/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
